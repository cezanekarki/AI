{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c814c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "\n",
    "#importing dataset\n",
    "boston=datasets.load_boston()\n",
    "X=boston['data']\n",
    "y=boston['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10105f72",
   "metadata": {},
   "source": [
    "Before building the RegularizedRegression class, let’s define a few helper functions. \n",
    "The first function standardizes the data by removing the mean and dividing by the standard deviation. \n",
    "This is the equivalent of the StandardScaler from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0e28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(X):\n",
    "    mean=X.mean(0)\n",
    "    stds=X.std(0)\n",
    "    return (X-mean)/stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c6551",
   "metadata": {},
   "source": [
    "The sign function simply returns the sign of each element in an array. \n",
    "This is useful for calculating the gradient in Lasso regression. \n",
    "The first_element_zero option makes the function return a 0 (rather than a -1 or 1) for the first element. \n",
    "As discussed in the concept section, this prevents Lasso regression from penalizing the magnitude of the intercept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e5bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x, first_element_zero=False):\n",
    "    signs=(-1)**(x<0)\n",
    "    if first_element_zero:\n",
    "        signs[0]=0\n",
    "    return signs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120cbd4",
   "metadata": {},
   "source": [
    "The RegularizedRegression class below contains methods for fitting Ridge and Lasso regression. The first method, record_info, handles standardization, adds an intercept to the predictors, and records the necessary values. The second, fit_ridge, fits Ridge regression using\n",
    "\n",
    "β^=(X⊤X+λI′)−1X⊤y.\n",
    "\n",
    "The third method, fit_lasso, estimates the regression parameters using gradient descent. The gradient is the derivative of the Lasso loss function:\n",
    "\n",
    "∂L(β^)∂β^=−X⊤(y−Xβ^)+λI′ sign(β^).\n",
    "\n",
    "The gradient descent used here simply adjusts the parameters a fixed number of times (determined by n_iters). There many more efficient ways to implement gradient descent, though we use a simple implementation here to keep focus on Lasso regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e934a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedRegression:\n",
    "    def record_info(self,X,y,lam,intercept,standardize):\n",
    "        \n",
    "        #standardize\n",
    "        if standardize == True:\n",
    "            X=standard_scaler(X)\n",
    "        \n",
    "        #add intercept\n",
    "        if intercept==False:\n",
    "            ones=np.ones(len(X)).reshape(len(X),1) #column of ones\n",
    "            X=np.concatenate((ones,X),axis=1) #concatenating the target and ones variables\n",
    "            \n",
    "        \n",
    "        #record values\n",
    "        self.X=np.array(X)\n",
    "        self.y=np.array(y)\n",
    "        self.lam=lam\n",
    "        self.N,self.D=self.X.shape\n",
    "        \n",
    "    \n",
    "    def fit_ridge(self, X,y,lam=0, intercept=False, standardize=True):\n",
    "    \n",
    "        #record data and dimensions\n",
    "        self.record_info(X,y,lam,intercept,standardize)\n",
    "        \n",
    "        #estimate parameters\n",
    "        XtX = np.dot(self.X.T, self.X)\n",
    "        I_prime = np.eye(self.D)\n",
    "        I_prime[0,0] = 0 \n",
    "        XtX_plus_lam_inverse = np.linalg.inv(XtX + self.lam*I_prime)\n",
    "        Xty = np.dot(self.X.T, self.y)\n",
    "        self.beta_hats = np.dot(XtX_plus_lam_inverse, Xty)\n",
    "        \n",
    "        # get fitted values\n",
    "        self.y_hat = np.dot(self.X, self.beta_hats)\n",
    "        \n",
    "    def fit_lasso(self, X, y, lam = 0, n_iters = 2000,\n",
    "                  lr = 0.0001, intercept = False, standardize = True):\n",
    "        \n",
    "        # record data and dimensions\n",
    "        self.record_info(X, y, lam, intercept, standardize)\n",
    "        \n",
    "        # estimate parameters\n",
    "        beta_hats = np.random.randn(self.D)\n",
    "        for i in range(n_iters):\n",
    "            dL_dbeta = -self.X.T @ (self.y - (self.X @ beta_hats)) + self.lam*sign(beta_hats, True)\n",
    "            beta_hats -= lr*dL_dbeta \n",
    "        self.beta_hats = beta_hats\n",
    "        \n",
    "        # get fitted values\n",
    "        self.y_hat = np.dot(self.X, self.beta_hats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3bb108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set lambda\n",
    "lam=10\n",
    "\n",
    "#fit ridge\n",
    "ridge_model=RegularizedRegression()\n",
    "ridge_model.fit_ridge(X,y,lam)\n",
    "\n",
    "#fit lass\n",
    "lasso_model=RegularizedRegression()\n",
    "lasso_model.fit_lasso(X,y,lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d2b10",
   "metadata": {},
   "source": [
    "The below graphic shows the coefficient estimates using Ridge and Lasso regression with a changing value of λ. Note that λ=0 is identical to ordinary linear regression. As expected, the magnitude of the coefficient estimates decreases as λ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f21acf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using python lib\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "alpha=1\n",
    "ridge_model=Ridge(alpha=alpha)\n",
    "ridge_model.fit(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "140705ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model=Lasso(alpha=alpha)\n",
    "lasso_model.fit(X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60794dea",
   "metadata": {},
   "source": [
    "In practice, however, we want to choose alpha through cross validation. This is easily implemented in scikit-learn by designating a set of alpha values to try and fitting the model with RidgeCV or LassoCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a21ecd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge alpha: 0.01\n",
      "Lasso alpha: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "alphas = [0.01, 1, 100]\n",
    "\n",
    "# Ridge\n",
    "ridgeCV_model = RidgeCV(alphas = alphas)\n",
    "ridgeCV_model.fit(X, y)\n",
    "\n",
    "# Lasso\n",
    "lassoCV_model = LassoCV(alphas = alphas)\n",
    "lassoCV_model.fit(X, y);\n",
    "\n",
    "print('Ridge alpha:', ridgeCV_model.alpha_)\n",
    "print('Lasso alpha:', lassoCV_model.alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fc44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
